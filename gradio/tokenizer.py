import json
import os


class BengaliBPETokenizer:
    """
    Simple byte-level BPE tokenizer for Bengali.
    Compatible with artifacts (merges.txt, vocab.json, config.json)
    generated by the Colab BPE training script (2 MB corpus, vocab = 5010).
    """

    def __init__(self, tokenizer_dir="bengali_tokenizer"):
        self.tokenizer_dir = tokenizer_dir
        self.byte_vocab_size = 256

        # --- Load assets ---
        self.vocab = self._load_vocab()
        self.merges = self._load_merges()
        self.config = self._load_config()

        # Reverse merge mapping (for decoding)
        self.reverse_merges = {
            self.byte_vocab_size + i: pair for i, pair in enumerate(self.merges)
        }

        print(
            f"✅ BengaliBPETokenizer initialized | "
            f"Vocab size: {self.config.get('vocab_size', len(self.vocab))} | "
            f"Merges: {len(self.merges)}"
        )

    # ---------------------------------------------------------------------
    # Internal loaders
    # ---------------------------------------------------------------------
    def _load_vocab(self):
        vocab_path = os.path.join(self.tokenizer_dir, "vocab.json")
        if not os.path.exists(vocab_path):
            raise FileNotFoundError(f"❌ vocab.json not found at: {vocab_path}")
        with open(vocab_path, "r", encoding="utf-8") as f:
            data = json.load(f)
        if not isinstance(data, dict):
            raise ValueError(
                "vocab.json must be a JSON object mapping token-id strings to token strings."
            )
        return data

    def _load_merges(self):
        merges_path = os.path.join(self.tokenizer_dir, "merges.txt")
        if not os.path.exists(merges_path):
            raise FileNotFoundError(f"❌ merges.txt not found at: {merges_path}")
        merges = []
        with open(merges_path, "r", encoding="utf-8") as f:
            for lineno, line in enumerate(f, start=1):
                line = line.strip()
                if not line or line.startswith("#"):
                    continue
                parts = line.split()
                if len(parts) != 2:
                    raise ValueError(f"Invalid merges.txt line {lineno}: {line}")
                try:
                    a, b = int(parts[0]), int(parts[1])
                except ValueError:
                    raise ValueError(f"Non-integer merge at line {lineno}: {line}")
                merges.append((a, b))
        return merges

    def _load_config(self):
        config_path = os.path.join(self.tokenizer_dir, "config.json")
        if not os.path.exists(config_path):
            return {}
        with open(config_path, "r", encoding="utf-8") as f:
            return json.load(f)

    # ---------------------------------------------------------------------
    # Core merge operation (same as training)
    # ---------------------------------------------------------------------
    def _merge_once(self, ids, pair, idx):
        new_ids = []
        i = 0
        while i < len(ids):
            if i < len(ids) - 1 and ids[i] == pair[0] and ids[i + 1] == pair[1]:
                new_ids.append(idx)
                i += 2
            else:
                new_ids.append(ids[i])
                i += 1
        return new_ids

    # ---------------------------------------------------------------------
    # Encoding: text → token IDs
    # ---------------------------------------------------------------------
    def encode(self, text):
        if not isinstance(text, str):
            raise ValueError("Input to encode() must be a string.")

        # Convert text to UTF-8 bytes
        ids = list(text.encode("utf-8"))

        # Apply merges in learned order
        for idx, pair in enumerate(self.merges, start=self.byte_vocab_size):
            ids = self._merge_once(ids, pair, idx)
        return ids

    # ---------------------------------------------------------------------
    # Recursive decode helpers
    # ---------------------------------------------------------------------
    def _expand_token(self, i):
        if i < self.byte_vocab_size:
            return [i]
        elif i in self.reverse_merges:
            a, b = self.reverse_merges[i]
            return self._expand_token(a) + self._expand_token(b)
        else:
            tok = self.vocab.get(str(i), "")
            return list(tok.encode("utf-8", errors="ignore")) if tok else []

    # ---------------------------------------------------------------------
    # Decoding: token IDs → text
    # ---------------------------------------------------------------------
    def decode(self, ids):
        if not isinstance(ids, (list, tuple)):
            raise ValueError("Input to decode() must be a list or tuple of integers.")

        expanded = []
        for i in ids:
            expanded.extend(self._expand_token(i))

        # keep only valid byte values
        expanded = [b for b in expanded if 0 <= b < 256]
        return bytes(expanded).decode("utf-8", errors="ignore")
