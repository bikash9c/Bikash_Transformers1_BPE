{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# --- Mount Google Drive ---\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# import os\n",
        "\n",
        "# # --- Create output folder inside Drive ---\n",
        "# save_dir = '/content/drive/MyDrive/bengali_corpus'\n",
        "# os.makedirs(save_dir, exist_ok=True)\n"
      ],
      "metadata": {
        "id": "0_KC9miuqmbX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e7a03df0-9a23-4f28-c38d-da356c7835b0"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# # --- File path in Drive ---\n",
        "# output_file = os.path.join(save_dir, 'bengali_corpus.txt')\n",
        "\n",
        "# # --- Load Bengali Wikipedia dataset ---\n",
        "# from datasets import load_dataset\n",
        "# from tqdm import tqdm\n",
        "# import re\n",
        "# import os\n",
        "\n",
        "# wiki = load_dataset(\"wikimedia/wikipedia\", \"20231101.bn\", split=\"train\")\n",
        "\n",
        "# min_length = 100  # skip very short articles\n",
        "\n",
        "# with open(output_file, 'w', encoding='utf-8') as f:\n",
        "#     for article in tqdm(wiki, desc=\"Saving articles\"):\n",
        "#         text = article['text'].strip()\n",
        "#         if len(text) < min_length:\n",
        "#             continue\n",
        "\n",
        "#         # basic cleaning\n",
        "#         text = re.sub(r'\\[\\d+\\]', '', text)   # remove [1], [2], etc.\n",
        "#         text = re.sub(r'\\n+', '\\n', text)     # collapse newlines\n",
        "\n",
        "#         f.write(text + '\\n\\n')\n",
        "\n",
        "# # --- Show stats ---\n",
        "# size_mb = os.path.getsize(output_file) / (1024**2)\n",
        "# print(f\"Saved corpus to: {output_file}\")\n",
        "# print(f\"Corpus size: {size_mb:.2f} MB\")"
      ],
      "metadata": {
        "id": "PbBTaiEED1_k"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "corpus_path = '/content/drive/MyDrive/bengali_corpus/bengali_corpus.txt'\n",
        "\n",
        "with open(corpus_path, 'r', encoding='utf-8') as f:\n",
        "    for _ in range(10):  # print first 10 lines\n",
        "        print(f.readline().strip())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "33WrWzCMsMnO",
        "outputId": "f1d9c797-9bcd-4536-b9a1-6e778721b1f1"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ржмрж╛ржВрж▓рж╛ ржнрж╛рж╖рж╛ (ржмрж╛ржЩрж▓рж╛, ржмрж╛ржЩрзНржЧрж▓рж╛, рждржерж╛ ржмрж╛ржЩрзНржЧрж╛рж▓рж╛ ржирж╛ржорзЗржУ ржкрж░рж┐ржЪрж┐ржд) ржПржХржЯрж┐ ржЗржирзНржжрзЛ-ржЖрж░рзНржп ржнрж╛рж╖рж╛, ржпрж╛ ржжржХрзНрж╖рж┐ржг ржПрж╢рж┐ржпрж╝рж╛рж░ ржмрж╛ржЩрж╛рж▓рж┐ ржЬрж╛рждрж┐рж░ ржкрзНрж░ржзрж╛ржи ржХржерзНржп ржУ рж▓рзЗржЦрзНржп ржнрж╛рж╖рж╛ред ржорж╛рждрзГржнрж╛рж╖рзАрж░ рж╕ржВржЦрзНржпрж╛ржпрж╝ ржмрж╛ржВрж▓рж╛ ржЗржирзНржжрзЛ-ржЗржЙрж░рзЛржкрзАржпрж╝ ржнрж╛рж╖рж╛ ржкрж░рж┐ржмрж╛рж░рзЗрж░ ржкржЮрзНржЪржо ржУ ржорзЛржЯ ржмрзНржпржмрж╣рж╛рж░ржХрж╛рж░рзАрж░ рж╕ржВржЦрзНржпрж╛ ржЕржирзБрж╕рж╛рж░рзЗ ржмрж╛ржВрж▓рж╛ ржмрж┐рж╢рзНржмрзЗрж░ рж╖рж╖рзНржа ржмрзГрж╣рждрзНрждржо ржнрж╛рж╖рж╛ред ржмрж╛ржВрж▓рж╛ рж╕рж╛рж░рзНржмржнрзМржо ржнрж╛рж╖рж╛ржнрж┐рждрзНрждрж┐ржХ ржЬрж╛рждрж┐рж░рж╛рж╖рзНржЯрзНрж░ ржмрж╛ржВрж▓рж╛ржжрзЗрж╢рзЗрж░ ржПржХржорж╛рждрзНрж░ рж░рж╛рж╖рзНржЯрзНрж░ржнрж╛рж╖рж╛ рждржерж╛ рж╕рж░ржХрж╛рж░рж┐ ржнрж╛рж╖рж╛ ржПржмржВ ржнрж╛рж░рждрзЗрж░ ржкрж╢рзНржЪрж┐ржоржмржЩрзНржЧ, рждрзНрж░рж┐ржкрзБрж░рж╛, ржЖрж╕рж╛ржорзЗрж░ ржмрж░рж╛ржХ ржЙржкрждрзНржпржХрж╛рж░ рж╕рж░ржХрж╛рж░рж┐ ржнрж╛рж╖рж╛ред ржмржЩрзНржЧрзЛржкрж╕рж╛ржЧрж░рзЗ ржЕржмрж╕рзНржерж┐ржд ржЖржирзНржжрж╛ржорж╛ржи ржжрзНржмрзАржкржкрзБржЮрзНржЬрзЗрж░ ржкрзНрж░ржзрж╛ржи ржХржерзНржп ржнрж╛рж╖рж╛ ржмрж╛ржВрж▓рж╛ред ржПржЫрж╛ржбрж╝рж╛ ржнрж╛рж░рждрзЗрж░ ржЭрж╛ржбрж╝ржЦржгрзНржб, ржмрж┐рж╣рж╛рж░, ржорзЗржШрж╛рж▓ржпрж╝, ржорж┐ржЬрзЛрж░рж╛ржо,  ржУржбрж╝рж┐рж╢рж╛ рж░рж╛ржЬрзНржпржЧрзБрж▓рзЛрждрзЗ ржЙрж▓рзНрж▓рзЗржЦржпрзЛржЧрзНржп ржкрж░рж┐ржорж╛ржгрзЗ ржмрж╛ржВрж▓рж╛ржнрж╛рж╖рзА ржЬржиржЧржг рж░ржпрж╝рзЗржЫрзЗред рзирзжрззрзз рж╕рж╛рж▓рзЗрж░ ржЖржжржорж╢рзБржорж╛рж░рж┐ ржЕржирзБржпрж╛ржпрж╝рзА, ржнрж╛рж░рждрзЗрж░ ржорзЛржЯ ржЬржирж╕ржВржЦрзНржпрж╛рж░ рзо.рзжрзй рж╢рждрж╛ржВрж╢ ржорж╛ржирзБрж╖ ржмрж╛ржВрж▓рж╛ ржнрж╛рж╖рж╛ржпрж╝ ржХржерж╛ ржмрж▓рзЗ ржПржмржВ рж╣рж┐ржирзНржжрж┐рж░ ржкрж░рзЗржЗ ржнрж╛рж░рждрзЗ рж╕рж░рзНржмрж╛ржзрж┐ржХ ржкрзНрж░ржЪрж▓рж┐ржд ржнрж╛рж╖рж╛ - ржмрж╛ржВрж▓рж╛ред ржПржЫрж╛ржбрж╝рж╛ржУ ржоржзрзНржп ржкрзНрж░рж╛ржЪрзНржп, ржЖржорзЗрж░рж┐ржХрж╛ ржУ ржЗржЙрж░рзЛржкрзЗ ржЙрж▓рзНрж▓рзЗржЦржпрзЛржЧрзНржп ржкрж░рж┐ржорж╛ржгрзЗ ржмрж╛ржВрж▓рж╛ржнрж╛рж╖рзА ржЕржнрж┐ржмрж╛рж╕рзА рж░ржпрж╝рзЗржЫрзЗред рж╕рж╛рж░рж╛ ржмрж┐рж╢рзНржмрзЗ рж╕ржм ржорж┐рж▓рж┐ржпрж╝рзЗ рзирзн.рзм ржХрзЛржЯрж┐рж░ ржЕржзрж┐ржХ рж▓рзЛржХ ржжрзИржиржирзНржжрж┐ржи ржЬрзАржмржирзЗ ржмрж╛ржВрж▓рж╛ ржмрзНржпржмрж╣рж╛рж░ ржХрж░рзЗред ржмрж╛ржВрж▓рж╛ржжрзЗрж╢рзЗрж░ ржЬрж╛рждрзАржпрж╝ рж╕ржЩрзНржЧрзАржд ржПржмржВ ржнрж╛рж░рждрзЗрж░ ржЬрж╛рждрзАржпрж╝ рж╕ржЩрзНржЧрзАржд ржУ рж╕рзНрждрзЛрждрзНрж░ ржмрж╛ржВрж▓рж╛рждрзЗ рж░ржЪрж┐рждред\n",
            "ржмрж╛ржВрж▓рж╛ ржнрж╛рж╖рж╛ ржмрж┐ржХрж╛рж╢рзЗрж░ ржЗрждрж┐рж╣рж╛рж╕ рззрзйрзжрзж ржмржЫрж░ ржкрзБрж░ржирзЛред ржЪрж░рзНржпрж╛ржкржж ржП ржнрж╛рж╖рж╛рж░ ржЖржжрж┐ ржирж┐ржжрж░рзНрж╢ржиред ржЕрж╖рзНржЯржо рж╢рждржХ ржерзЗржХрзЗ ржмрж╛ржВрж▓рж╛ржпрж╝ рж░ржЪрж┐ржд рж╕рж╛рж╣рж┐рждрзНржпрзЗрж░ ржмрж┐рж╢рж╛рж▓ ржнрж╛ржиржбрж╛рж░рзЗрж░ ржоржзрзНржп ржжрж┐ржпрж╝рзЗ ржЕрж╖рзНржЯрж╛ржжрж╢ рж╢рждржХрзЗрж░ рж╢рзЗрж╖рзЗ ржПрж╕рзЗ ржмрж╛ржВрж▓рж╛ ржнрж╛рж╖рж╛ рждрж╛рж░ ржмрж░рзНрждржорж╛ржи рж░рзВржк ржкрж░рж┐ржЧрзНрж░рж╣ржг ржХрж░рзЗред ржмрж╛ржВрж▓рж╛ ржнрж╛рж╖рж╛рж░ рж▓рж┐ржкрж┐ рж╣рж▓рзЛ ржмрж╛ржВрж▓рж╛ рж▓рж┐ржкрж┐ред ржмрж╛ржВрж▓рж╛ржжрзЗрж╢ ржУ ржкрж╢рзНржЪрж┐ржоржмржЩрзНржЧрзЗ ржкрзНрж░ржЪрж▓рж┐ржд ржмрж╛ржВрж▓рж╛ ржнрж╛рж╖рж╛рж░ ржоржзрзНржпрзЗ рж╢ржмрзНржжржЧржд ржУ ржЙржЪрзНржЪрж╛рж░ржгржЧржд рж╕рж╛ржорж╛ржирзНржп ржкрж╛рж░рзНржержХрзНржп рж░ржпрж╝рзЗржЫрзЗред ржмрж╛ржВрж▓рж╛рж░ ржиржмржЬрж╛ржЧрж░ржгрзЗ ржУ ржмрж╛ржВрж▓рж╛рж░ рж╕рж╛ржВрж╕рзНржХрзГрждрж┐ржХ ржмрж┐ржмрж┐ржзрждрж╛ржХрзЗ ржПржХ рж╕рзВрждрзНрж░рзЗ ржЧрзНрж░ржирзНржержирзЗ ржПржмржВ ржмрж╛ржЩрж╛рж▓рж┐ ржЬрж╛рждрзАржпрж╝рждрж╛ржмрж╛ржжрзЗрж░ ржмрж┐ржХрж╛рж╢рзЗ рждржерж╛ ржмрж╛ржВрж▓рж╛ржжрзЗрж╢ ржЧржаржирзЗ ржмрж╛ржВрж▓рж╛ ржнрж╛рж╖рж╛ ржУ рж╕рж╛рж╣рж┐рждрзНржп рж╕ржмржЪрзЗржпрж╝рзЗ ржЧрзБрж░рзБрждрзНржмржкрзВрж░рзНржг ржнрзВржорж┐ржХрж╛ рж░рзЗржЦрзЗржЫрзЗред\n",
            "рззрзпрзкрзн ржерзЗржХрзЗ рззрзпрзлрзи ржЦрзНрж░рж┐рж╖рзНржЯрж╛ржмрзНржжрзЗ ржкрзВрж░рзНржм ржмрж╛ржВрж▓рж╛ржпрж╝ рж╕ржВржЧржарж┐ржд ржмрж╛ржВрж▓рж╛ ржнрж╛рж╖рж╛ ржЖржирзНржжрзЛрж▓ржи ржПржЗ ржнрж╛рж╖рж╛рж░ рж╕рж╛ржерзЗ ржмрж╛ржЩрж╛рж▓рж┐ ржЕрж╕рзНрждрж┐рждрзНржмрзЗрж░ ржпрзЛржЧрж╕рзВрждрзНрж░ рж╕рзНржерж╛ржкржи ржХрж░рзЗржЫрзЗред рззрзпрзлрзи ржЦрзНрж░рж┐рж╖рзНржЯрж╛ржмрзНржжрзЗрж░ рзирззрж╢рзЗ ржлрзЗржмрзНрж░рзБржпрж╝рж╛рж░рж┐ ржврж╛ржХрж╛ ржмрж┐рж╢рзНржмржмрж┐ржжрзНржпрж╛рж▓ржпрж╝рзЗ ржкрзНрж░рждрж┐ржмрж╛ржжрзА ржЫрж╛рждрзНрж░ ржУ ржЖржирзНржжрзЛрж▓ржиржХрж╛рж░рзАрж░рж╛ рж╕ржВржЦрзНржпрж╛ржЧрж░рж┐рж╖рзНржарзЗрж░ ржорж╛рждрзГржнрж╛рж╖рж╛ ржмрж╛ржВрж▓рж╛ржХрзЗ рж░рж╛рж╖рзНржЯрзНрж░ржнрж╛рж╖рж╛ржХрж░ржгрзЗрж░ ржжрж╛ржмрж┐рждрзЗ ржирж┐ржЬрзЗржжрзЗрж░ ржЬрзАржмржи ржЙрзОрж╕рж░рзНржЧ ржХрж░рзЗржиред рззрзпрзорзн рж╕рж╛рж▓рзЗрж░ ржмрж╛ржВрж▓рж╛ ржнрж╛рж╖рж╛ ржкрзНрж░ржЪрж▓ржи ржЖржЗржи ржмрж╛ржВрж▓рж╛ржжрзЗрж╢рзЗрж░ рж╕ржХрж▓ рж░рж╛рж╖рзНржЯрзНрж░рзАржпрж╝ ржХрж╛ржЬрзЗ ржмрж╛ржВрж▓рж╛рж░ ржмрзНржпржмрж╣рж╛рж░ ржмрж╛ржзрзНржпрждрж╛ржорзВрж▓ржХ ржХрж░рж╛ рж╣ржпрж╝рзЗржЫрзЗред рззрзпрзлрзи рж╕рж╛рж▓рзЗрж░ ржнрж╛рж╖рж╛ рж╢рж╣рж┐ржжржжрзЗрж░ рж╕ржВржЧрзНрж░рж╛ржорзЗрж░ рж╕рзНржмрзАржХрзГрждрж┐ рж╕рзНржмрж░рзВржк рззрзпрзпрзп ржЦрзНрж░рж┐рж╖рзНржЯрж╛ржмрзНржжрзЗ ржЗржЙржирзЗрж╕рзНржХрзЛ рзирззрж╢рзЗ ржлрзЗржмрзНрж░рзБржпрж╝рж╛рж░рж┐ ржжрж┐ржиржЯрж┐ржХрзЗ ржЖржирзНрждрж░рзНржЬрж╛рждрж┐ржХ ржорж╛рждрзГржнрж╛рж╖рж╛ ржжрж┐ржмрж╕ рж╣рж┐рж╕рзЗржмрзЗ ржШрзЛрж╖ржгрж╛ ржХрж░рзЗред\n",
            "ржЗрждрж┐рж╣рж╛рж╕\n",
            "ржмрж╛ржВрж▓рж╛ ржнрж╛рж╖рж╛рж░ ржЗрждрж┐рж╣рж╛рж╕ржХрзЗ рж╕рж╛ржзрж╛рж░ржгржд рждрж┐ржи ржнрж╛ржЧрзЗ ржнрж╛ржЧ ржХрж░рж╛ рж╣ржпрж╝:\n",
            "ржкрзНрж░рж╛ржЪрзАржи ржмрж╛ржВрж▓рж╛ (рзпрзжрзж/рззрзжрзжрзж-рззрзкрзжрзж ржЦрзНрж░рж┐рж╖рзНржЯрж╛ржмрзНржж) тАФ ржЪрж░рзНржпрж╛ржкржж, ржнржХрзНрждрж┐ржорзВрж▓ржХ ржЧрж╛ржи ржПржЗ рж╕ржоржпрж╝ржХрж╛рж░ рж▓рж┐ржЦрж┐ржд ржирж┐ржжрж░рзНрж╢ржиред ржПржЗ рж╕ржоржпрж╝ ржЖржорж┐, рждрзБржорж┐ ржЗрждрзНржпрж╛ржжрж┐ рж╕рж░рзНржмржирж╛ржо ржПржмржВ -ржЗрж▓рж╛, -ржЗржмрж╛, ржЗрждрзНржпрж╛ржжрж┐ ржХрзНрж░рж┐ржпрж╝рж╛ржмрж┐ржнржХрзНрждрж┐рж░ ржЖржмрж┐рж░рзНржнрж╛ржм ржШржЯрзЗред\n",
            "ржоржзрзНржп ржмрж╛ржВрж▓рж╛ (рззрзкрзжрзж-рззрзорзжрзж ржЦрзНрж░рж┐рж╖рзНржЯрж╛ржмрзНржж) тАФ ржП рж╕ржоржпрж╝ржХрж╛рж░ ржЧрзБрж░рзБрждрзНржмржкрзВрж░рзНржг рж▓рж┐ржЦрж┐ржд ржирж┐ржжрж░рзНрж╢ржи ржЪржгрзНржбрзАржжрж╛рж╕рзЗрж░ рж╢рзНрж░рзАржХрзГрж╖рзНржгржХрзАрж░рзНрждржи ржЗрждрзНржпрж╛ржжрж┐ред рж╢ржмрзНржжрзЗрж░ рж╢рзЗрж╖рзЗ \"ржЕ\" ржзрзНржмржирж┐рж░ ржмрж┐рж▓рзЛржк, ржпрзМржЧрж┐ржХ ржХрзНрж░рж┐ржпрж╝рж╛рж░ ржкрзНрж░ржЪрж▓ржи, ржлрж╛рж░рж╕рж┐ ржнрж╛рж╖рж╛рж░ ржкрзНрж░ржнрж╛ржм ржПржЗ рж╕ржоржпрж╝рзЗрж░ рж╕рж╛рж╣рж┐рждрзНржпрзЗ рж▓ржХрзНрж╖рзНржп ржХрж░рж╛ ржпрж╛ржпрж╝ред ржХрзЛржирзЛ ржХрзЛржирзЛ ржнрж╛рж╖рж╛ржмрж┐ржж ржПржЗ ржпрзБржЧржХрзЗ ржЖржжрж┐ ржУ ржЕржирзНрждрзНржп ржПржЗ ржжрзБржЗ ржнрж╛ржЧрзЗ ржнрж╛ржЧ ржХрж░рзЗржиред\n",
            "ржЖржзрзБржирж┐ржХ ржмрж╛ржВрж▓рж╛ (рззрзорзжрзж ржЦрзНрж░рж┐рж╖рзНржЯрж╛ржмрзНржж ржерзЗржХрзЗ-ржмрж░рзНрждржорж╛ржи) тАФ ржПржЗ рж╕ржоржпрж╝ ржХрзНрж░рж┐ржпрж╝рж╛ ржУ рж╕рж░рзНржмржирж╛ржорзЗрж░ рж╕ржВржХрзНрж╖рзЗржкржг ржШржЯрзЗ, ржпрзЗржоржи рждрж╛рж╣рж╛рж░ тЖТ рждрж╛рж░; ржХрж░рж┐ржпрж╝рж╛ржЫрж┐рж▓ тЖТ ржХрж░рзЗржЫрж┐рж▓ред\n",
            "ржмрж╛ржВрж▓рж╛рж░ ржкрзНрж░рж╛ржЪрзАржи ржнрж╛рж╖рж╛\n",
            "ржЦрзНрж░рж┐рж╖рзНржЯржкрзВрж░рзНржм ржкрзНрж░ржержо рж╕рж╣рж╕рзНрж░рж╛ржмрзНржж ржерзЗржХрзЗ ржмрж╛ржВрж▓рж╛ржпрж╝ рж╣рж┐ржирзНржжрзБ ржмрзНрж░рж╛рж╣рзНржоржгржЧржг рж╕ржВрж╕рзНржХрзГржд ржнрж╛рж╖рж╛рж░ ржЪрж░рзНржЪрж╛ ржХрж░ржд, ржХрж┐ржирзНрждрзБ рж╕рзНржерж╛ржирзАржпрж╝ ржмрзМржжрзНржзрж░рж╛ ржкрзНрж░рж╛ржХрзГржд ржнрж╛рж╖рж╛рж░ ржХрзЛржи ржХрзЛржи рж░рзВржкрзЗ (ржнрзНржпрж╛рж░рж╛ржЗржЯрж┐) ржХржерж╛ ржмрж▓ржд, ржпрж╛ржХрзЗ ржбржГ рж╕рзБржирзАрждрж┐ ржХрзБржорж╛рж░ ржЪржЯрзНржЯрзЛржкрж╛ржзрзНржпрж╛ржпрж╝ ржЙрж▓рзНрж▓рзЗржЦ ржХрж░рзЗржЫрзЗржи ржорж╛ржЧржзрзА ржкрзНрж░рж╛ржХрзГрждрзЗрж░ ржкрзВрж░рзНржм рж░рзВржк ржмрж╛ ржнрзНржпрж╛рж░рж╛ржЗржЯрж┐ рж╣рж┐рж╕рзЗржмрзЗред ржЧрзБржкрзНржд рж╕рж╛ржорзНрж░рж╛ржЬрзНржпрзЗрж░ рж╕ржоржпрж╝, ржмрж╛ржВрж▓рж╛ ржЫрж┐рж▓ рж╣рж┐ржирзНржжрзБ ржпрж╛ржЬржХ ржмрж╛ ржкрзБрж░рзЛрж╣рж┐рждржжрзЗрж░ ржЬржирзНржп рж╕ржВрж╕рзНржХрзГржд рж╕рж╛рж╣рж┐рждрзНржпрзЗрж░ ржПржХржЯрж┐ ржХрзЗржирзНржжрзНрж░, ржпрж╛ рж╕рзНржерж╛ржирзАржпрж╝ржжрзЗрж░ ржХржерзНржп ржнрж╛рж╖рж╛ржХрзЗ ржкрзНрж░ржнрж╛ржмрж┐ржд ржХрж░рзЗред ржкрзНрж░ржержо рж╕рж╣рж╕рзНрж░рж╛ржмрзНржжрзЗ ржмрж╛ржВрж▓рж╛ ржпржЦржи ржоржЧржз рж░рж╛ржЬрзНржпрзЗрж░ ржПржХржЯрж┐ ржЕржВрж╢ ржЫрж┐рж▓ рждржЦржи ржоржзрзНржп ржЗржирзНржжрзЛ-ржЖрж░рзНржп ржЙржкржнрж╛рж╖рж╛ржЧрзБрж▓рж┐ ржмрж╛ржВрж▓рж╛ржпрж╝ ржкрзНрж░ржнрж╛ржмрж╢рж╛рж▓рзА ржЫрж┐рж▓ред ржПржЗ ржЙржкржнрж╛рж╖рж╛ржЧрзБрж▓рж┐ржХрзЗ ржорж╛ржЧржзрзА ржкрзНрж░рж╛ржХрзГржд ржмрж▓рж╛ рж╣ржпрж╝ ржПржмржВ ржПржЯрж┐ ржЖржзрзБржирж┐ржХ ржмрж┐рж╣рж╛рж░, ржмрж╛ржВрж▓рж╛ ржУ ржЖрж╕рж╛ржорзЗ ржХржерж┐ржд рж╣рждред ржПржЗ ржнрж╛рж╖рж╛ ржерзЗржХрзЗ ржЕржмрж╢рзЗрж╖рзЗ ржЕрж░рзНржз-ржорж╛ржЧржзрзА ржкрзНрж░рж╛ржХрзГрждрзЗрж░ ржмрж┐ржХрж╛рж╢ ржШржЯрзЗред ржкрзНрж░ржержо рж╕рж╣рж╕рзНрж░рж╛ржмрзНржжрзЗрж░ рж╢рзЗрж╖рзЗрж░ ржжрж┐ржХрзЗ ржЕрж░рзНржз-ржорж╛ржЧржзрзА ржерзЗржХрзЗ ржЕржкржнрзНрж░ржВрж╢рзЗрж░ ржмрж┐ржХрж╛рж╢ ржШржЯрзЗред рж╕ржоржпрж╝рзЗрж░ рж╕рж╛ржерзЗ рж╕рж╛ржерзЗ ржмрж╛ржВрж▓рж╛ ржнрж╛рж╖рж╛ ржПржХржЯрж┐ рж╕рзНржмрждржирзНрждрзНрж░ ржнрж╛рж╖рж╛ рж╣рж┐рж╕рзЗржмрзЗ ржмрж┐ржХрж╢рж┐ржд рж╣ржпрж╝ред\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # with open('bengali_corpus.txt', 'r', encoding='utf-8') as f:\n",
        "# #     articles = f.read().split('\\n\\n')  # each article is one element\n",
        "\n",
        "# # print(f\"Total articles: {len(articles)}\")\n",
        "# # print(\"\\n--- First article ---\\n\")\n",
        "# # print(articles[0])\n",
        "# num_chars = sum(len(a) for a in articles)\n",
        "# num_tokens_approx = sum(len(a.split()) for a in articles)  # rough word count\n",
        "\n",
        "# print(f\"Total characters: {num_chars}\")\n",
        "# print(f\"Approx. total tokens: {num_tokens_approx}\")"
      ],
      "metadata": {
        "id": "W0Zn5bnWuVO0"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(corpus_path, \"r\", encoding=\"utf-8\") as f:\n",
        "    text = f.read()"
      ],
      "metadata": {
        "id": "x6hJNoLYx2Mx"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Byte-level encoding\n",
        "tokens = list(text.encode(\"utf-8\"))  # each byte 0тАУ255\n",
        "print(\"Initial tokens:\", len(tokens))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vxpLBCl9uqe9",
        "outputId": "8d89864c-c848-46ce-dc6c-eb2678039376"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial tokens: 926042697\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import os\n",
        "import json\n",
        "from tqdm import tqdm\n",
        "\n",
        "# --- 1я╕ПтГг Load only 5 MB of data ---\n",
        "corpus_path = '/content/drive/MyDrive/bengali_corpus/bengali_corpus.txt'\n",
        "\n",
        "with open(corpus_path, \"r\", encoding=\"utf-8\") as f:\n",
        "    text = f.read()\n",
        "\n",
        "sample_bytes = text.encode(\"utf-8\")[:5 * 1024 * 1024]\n",
        "tokens = list(sample_bytes)\n",
        "print(f\"Training on {len(tokens):,} byte tokens (~5 MB of data)\")\n",
        "\n",
        "# --- 2я╕ПтГг Define helper functions ---\n",
        "def get_stats(ids):\n",
        "    counts = {}\n",
        "    for pair in zip(ids, ids[1:]):\n",
        "        counts[pair] = counts.get(pair, 0) + 1\n",
        "    return counts\n",
        "\n",
        "def merge(ids, pair, idx):\n",
        "    newids = []\n",
        "    i = 0\n",
        "    while i < len(ids):\n",
        "        if i < len(ids) - 1 and ids[i] == pair[0] and ids[i + 1] == pair[1]:\n",
        "            newids.append(idx)\n",
        "            i += 2\n",
        "        else:\n",
        "            newids.append(ids[i])\n",
        "            i += 1\n",
        "    return newids\n",
        "\n",
        "# --- 3я╕ПтГг Train BPE (recompute every merge) ---\n",
        "vocab_size = 500           # target vocab size\n",
        "num_merges = vocab_size - 256\n",
        "ids = list(tokens)\n",
        "\n",
        "print(f\"\\nЁЯЪА Starting BPE training: {num_merges} merges (full recomputation each step)...\\n\")\n",
        "\n",
        "merges = []  # track all merges in order (each is a tuple of ints)\n",
        "with tqdm(total=num_merges, desc=\"BPE Training Progress\", ncols=100) as pbar:\n",
        "    for merge_counter in range(num_merges):\n",
        "        stats = get_stats(ids)\n",
        "        if not stats:\n",
        "            print(\"тЪая╕П No more valid pairs to merge тАФ stopping early.\")\n",
        "            break\n",
        "\n",
        "        pair = max(stats, key=stats.get)\n",
        "        idx = 256 + merge_counter\n",
        "        ids = merge(ids, pair, idx)\n",
        "        merges.append(pair)\n",
        "        pbar.update(1)\n",
        "\n",
        "        if (merge_counter + 1) % 100 == 0 or merge_counter == num_merges - 1:\n",
        "            compression_ratio = len(tokens) / len(ids)\n",
        "            print(f\"ЁЯУж Merges: {merge_counter+1}/{num_merges} \"\n",
        "                  f\"| Current compression: {compression_ratio:.2f}├Ч \"\n",
        "                  f\"| Current length: {len(ids):,}\")\n",
        "\n",
        "# --- 4я╕ПтГг Final stats ---\n",
        "final_ratio = len(tokens) / len(ids)\n",
        "print(\"\\nтЬЕ Training complete!\")\n",
        "print(f\"тЬЕ Final vocabulary size: {vocab_size}\")\n",
        "print(f\"тЬЕ Final compression ratio: {final_ratio:.2f}├Ч\")\n",
        "\n",
        "# --- 5я╕ПтГг Save artifacts (merges.txt, vocab.json, config.json) ---\n",
        "save_dir = '/content/drive/MyDrive/bengali_tokenizer'\n",
        "os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "# 5.1 merges.txt\n",
        "merges_path = os.path.join(save_dir, 'merges.txt')\n",
        "with open(merges_path, 'w', encoding='utf-8') as f:\n",
        "    for a, b in merges:\n",
        "        f.write(f\"{a} {b}\\n\")\n",
        "\n",
        "# 5.2 vocab.json тАУ include readable mappings for all merged tokens\n",
        "vocab = {}\n",
        "\n",
        "# Base byte tokens (0тАУ255)\n",
        "for i in range(256):\n",
        "    vocab[str(i)] = chr(i)\n",
        "\n",
        "# Add merged tokens (тЙе256)\n",
        "for idx, (a, b) in enumerate(merges, start=256):\n",
        "    a_char = vocab.get(str(a), f\"<{a}>\")\n",
        "    b_char = vocab.get(str(b), f\"<{b}>\")\n",
        "    vocab[str(idx)] = a_char + b_char\n",
        "\n",
        "vocab[\"_note\"] = (\n",
        "    \"Base byte tokens 0тАУ255 stored above; merged tokens 256+ constructed \"\n",
        "    \"recursively from their parent pairs.\"\n",
        ")\n",
        "\n",
        "vocab_path = os.path.join(save_dir, 'vocab.json')\n",
        "with open(vocab_path, 'w', encoding='utf-8') as f:\n",
        "    json.dump(vocab, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "# 5.3 config.json\n",
        "config = {\n",
        "    \"vocab_size\": vocab_size,\n",
        "    \"num_merges_saved\": len(merges),\n",
        "    \"compression_ratio\": round(final_ratio, 4),\n",
        "    \"merges_start_index\": 256,\n",
        "    \"notes\": \"Byte-level BPE tokenizer for Bengali corpus; includes mappings for merged tokens >255.\"\n",
        "}\n",
        "config_path = os.path.join(save_dir, 'config.json')\n",
        "with open(config_path, 'w', encoding='utf-8') as f:\n",
        "    json.dump(config, f, indent=2)\n",
        "\n",
        "print(f\"\\nЁЯУБ Saved -> {merges_path}\")\n",
        "print(f\"ЁЯУБ Saved -> {vocab_path}\")\n",
        "print(f\"ЁЯУБ Saved -> {config_path}\")\n",
        "print(f\"тЬЕ All tokenizer artifacts saved in: {save_dir}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RSIy7YHkR7g2",
        "outputId": "679564f0-9602-49eb-b857-7a3e936d4f01"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training on 5,242,880 byte tokens (~5 MB of data)\n",
            "\n",
            "ЁЯЪА Starting BPE training: 244 merges (full recomputation each step)...\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "BPE Training Progress:  41%|тЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦМ                      | 100/244 [01:43<02:26,  1.02s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ЁЯУж Merges: 100/244 | Current compression: 3.42├Ч | Current length: 1,533,033\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "BPE Training Progress:  82%|тЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦП      | 200/244 [02:58<00:27,  1.60it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ЁЯУж Merges: 200/244 | Current compression: 4.04├Ч | Current length: 1,296,602\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "BPE Training Progress: 100%|тЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИ| 244/244 [03:28<00:00,  1.17it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ЁЯУж Merges: 244/244 | Current compression: 4.25├Ч | Current length: 1,234,765\n",
            "\n",
            "тЬЕ Training complete!\n",
            "тЬЕ Final vocabulary size: 500\n",
            "тЬЕ Final compression ratio: 4.25├Ч\n",
            "\n",
            "ЁЯУБ Saved -> /content/drive/MyDrive/bengali_tokenizer/merges.txt\n",
            "ЁЯУБ Saved -> /content/drive/MyDrive/bengali_tokenizer/vocab.json\n",
            "ЁЯУБ Saved -> /content/drive/MyDrive/bengali_tokenizer/config.json\n",
            "тЬЕ All tokenizer artifacts saved in: /content/drive/MyDrive/bengali_tokenizer\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- ЁЯФН Test: Verify Bengali Encode/Decode Roundtrip ---\n",
        "import os\n",
        "import json\n",
        "\n",
        "save_dir = '/content/drive/MyDrive/bengali_tokenizer'\n",
        "merges_path = os.path.join(save_dir, 'merges.txt')\n",
        "vocab_path = os.path.join(save_dir, 'vocab.json')\n",
        "\n",
        "# --- Load merges and vocab ---\n",
        "with open(merges_path, 'r', encoding='utf-8') as f:\n",
        "    merges = [tuple(map(int, line.strip().split())) for line in f if line.strip()]\n",
        "\n",
        "with open(vocab_path, 'r', encoding='utf-8') as f:\n",
        "    vocab = json.load(f)\n",
        "\n",
        "# --- Build reverse merge map ---\n",
        "reverse_merges = {256 + i: pair for i, pair in enumerate(merges)}\n",
        "\n",
        "# --- Merge function (same as training) ---\n",
        "def merge(ids, pair, idx):\n",
        "    newids = []\n",
        "    i = 0\n",
        "    while i < len(ids):\n",
        "        if i < len(ids) - 1 and ids[i] == pair[0] and ids[i + 1] == pair[1]:\n",
        "            newids.append(idx)\n",
        "            i += 2\n",
        "        else:\n",
        "            newids.append(ids[i])\n",
        "            i += 1\n",
        "    return newids\n",
        "\n",
        "# --- Encode ---\n",
        "def encode_text(s, merges):\n",
        "    ids = list(s.encode(\"utf-8\"))\n",
        "    for idx, pair in enumerate(merges, start=256):\n",
        "        ids = merge(ids, pair, idx)\n",
        "    return ids\n",
        "\n",
        "# --- Recursive expand helper ---\n",
        "def expand_token(i, reverse_merges):\n",
        "    if i < 256:\n",
        "        return [i]\n",
        "    elif i in reverse_merges:\n",
        "        a, b = reverse_merges[i]\n",
        "        return expand_token(a, reverse_merges) + expand_token(b, reverse_merges)\n",
        "    else:\n",
        "        return []\n",
        "\n",
        "# --- Decode ---\n",
        "def decode_ids(ids):\n",
        "    expanded = []\n",
        "    for i in ids:\n",
        "        expanded.extend(expand_token(i, reverse_merges))\n",
        "    expanded = [b for b in expanded if 0 <= b < 256]\n",
        "    return bytes(expanded).decode(\"utf-8\", errors=\"ignore\")\n",
        "\n",
        "# --- ЁЯзк Test with 10 Bengali examples ---\n",
        "test_sentences = [\n",
        "    \"ржЖржкржирж╛ржХрзЗ ржЕржирзЗржХ ржзржирзНржпржмрж╛ржжред\",                          # Thank you very much.\n",
        "    \"ржмрж╛ржВрж▓рж╛ ржнрж╛рж╖рж╛ ржЖржорж╛рж░ ржЧрж░рзНржмред\",                          # Bengali language is my pride.\n",
        "    \"ржЖржЬржХрзЗрж░ ржЖржмрж╣рж╛ржУржпрж╝рж╛ рж╕рзБржирзНржжрж░ред\",                        # Today's weather is nice.\n",
        "    \"ржЖржорж┐ ржмржЗ ржкржбрж╝рждрзЗ ржнрж╛рж▓рзЛржмрж╛рж╕рж┐ред\",                       # I love reading books.\n",
        "    \"рждрзБржорж┐ ржХрзЛржерж╛ржпрж╝ ржпрж╛ржЪрзНржЫрзЛ?\",                           # Where are you going?\n",
        "    \"ржПржЯрж╛ ржПржХржЯрж┐ ржЫрзЛржЯ ржЧрж▓рзНржкред\",                            # This is a short story.\n",
        "    \"рж╕рзЗ ржЦрзБржм ржкрж░рж┐рж╢рзНрж░ржорзА ржЫрж╛рждрзНрж░ред\",                        # He is a hardworking student.\n",
        "    \"ржЖржорж╛ржжрзЗрж░ рж╕рзНржХрзБрж▓ рж╢рж╣рж░рзЗ ржЕржмрж╕рзНржерж┐рждред\",                    # Our school is located in the city.\n",
        "    \"ржЦрж╛ржмрж╛рж░ржЯрж╛ ржжрж╛рж░рзБржг рж▓рзЗржЧрзЗржЫрзЗ!\",                         # The food was delicious!\n",
        "    \"ржЖржЧрж╛ржорзАржХрж╛рж▓ рж╕ржХрж╛рж▓рзЗ рж╕рзВрж░рзНржпрзЛржжржпрж╝ рж╣ржмрзЗред\"                 # The sunrise will happen tomorrow morning.\n",
        "]\n",
        "\n",
        "print(\"\\nЁЯФН Bengali Tokenizer Encode/Decode Roundtrip Test\\n\")\n",
        "\n",
        "for i, sentence in enumerate(test_sentences, start=1):\n",
        "    encoded = encode_text(sentence, merges)\n",
        "    decoded = decode_ids(encoded)\n",
        "    match = (decoded == sentence)\n",
        "\n",
        "    print(f\"ЁЯзк Example {i}:\")\n",
        "    print(f\"ЁЯУЭ Original: {sentence}\")\n",
        "    print(f\"ЁЯзй Encoded IDs: {encoded[:30]}{' ...' if len(encoded) > 30 else ''}\")\n",
        "    print(f\"ЁЯФБ Decoded: {decoded}\")\n",
        "    print(f\"тЬЕ Match: {match}\")\n",
        "    print(\"-\" * 80)\n",
        "\n",
        "# --- Summary ---\n",
        "total = len(test_sentences)\n",
        "passed = sum(decode_ids(encode_text(s, merges)) == s for s in test_sentences)\n",
        "print(f\"\\nЁЯОп Roundtrip Accuracy: {passed}/{total} ({100*passed/total:.1f}%)\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8WyhKYs8GFHv",
        "outputId": "f26cf50e-40d3-4a19-cf71-77563b79d647"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ЁЯФН Bengali Tokenizer Encode/Decode Roundtrip Test\n",
            "\n",
            "ЁЯзк Example 1:\n",
            "ЁЯУЭ Original: ржЖржкржирж╛ржХрзЗ ржЕржирзЗржХ ржзржирзНржпржмрж╛ржжред\n",
            "ЁЯзй Encoded IDs: [256, 134, 304, 268, 339, 439, 268, 347, 258, 167, 268, 443, 344, 318]\n",
            "ЁЯФБ Decoded: ржЖржкржирж╛ржХрзЗ ржЕржирзЗржХ ржзржирзНржпржмрж╛ржжред\n",
            "тЬЕ Match: True\n",
            "--------------------------------------------------------------------------------\n",
            "ЁЯзк Example 2:\n",
            "ЁЯУЭ Original: ржмрж╛ржВрж▓рж╛ ржнрж╛рж╖рж╛ ржЖржорж╛рж░ ржЧрж░рзНржмред\n",
            "ЁЯзй Encoded IDs: [278, 412, 277, 279, 173, 480, 279, 134, 285, 302, 151, 411, 318]\n",
            "ЁЯФБ Decoded: ржмрж╛ржВрж▓рж╛ ржнрж╛рж╖рж╛ ржЖржорж╛рж░ ржЧрж░рзНржмред\n",
            "тЬЕ Match: True\n",
            "--------------------------------------------------------------------------------\n",
            "ЁЯзк Example 3:\n",
            "ЁЯУЭ Original: ржЖржЬржХрзЗрж░ ржЖржмрж╣рж╛ржУржпрж╝рж╛ рж╕рзБржирзНржжрж░ред\n",
            "ЁЯзй Encoded IDs: [256, 134, 341, 276, 280, 134, 278, 311, 265, 147, 284, 485, 273, 357, 166, 260, 318]\n",
            "ЁЯФБ Decoded: ржЖржЬржХрзЗрж░ ржЖржмрж╣рж╛ржУржпрж╝рж╛ рж╕рзБржирзНржжрж░ред\n",
            "тЬЕ Match: True\n",
            "--------------------------------------------------------------------------------\n",
            "ЁЯзк Example 4:\n",
            "ЁЯУЭ Original: ржЖржорж┐ ржмржЗ ржкржбрж╝рждрзЗ ржнрж╛рж▓рзЛржмрж╛рж╕рж┐ред\n",
            "ЁЯзй Encoded IDs: [256, 134, 285, 290, 172, 333, 322, 442, 274, 270, 173, 308, 286, 278, 332, 264, 318]\n",
            "ЁЯФБ Decoded: ржЖржорж┐ ржмржЗ ржкржбрж╝рждрзЗ ржнрж╛рж▓рзЛржмрж╛рж╕рж┐ред\n",
            "тЬЕ Match: True\n",
            "--------------------------------------------------------------------------------\n",
            "ЁЯзк Example 5:\n",
            "ЁЯУЭ Original: рждрзБржорж┐ ржХрзЛржерж╛ржпрж╝ ржпрж╛ржЪрзНржЫрзЛ?\n",
            "ЁЯзй Encoded IDs: [274, 273, 285, 290, 149, 286, 381, 313, 405, 265, 154, 263, 155, 286, 63]\n",
            "ЁЯФБ Decoded: рждрзБржорж┐ ржХрзЛржерж╛ржпрж╝ ржпрж╛ржЪрзНржЫрзЛ?\n",
            "тЬЕ Match: True\n",
            "--------------------------------------------------------------------------------\n",
            "ЁЯзк Example 6:\n",
            "ЁЯУЭ Original: ржПржЯрж╛ ржПржХржЯрж┐ ржЫрзЛржЯ ржЧрж▓рзНржкред\n",
            "ЁЯзй Encoded IDs: [489, 299, 279, 436, 155, 286, 299, 393, 277, 395, 318]\n",
            "ЁЯФБ Decoded: ржПржЯрж╛ ржПржХржЯрж┐ ржЫрзЛржЯ ржЧрж▓рзНржкред\n",
            "тЬЕ Match: True\n",
            "--------------------------------------------------------------------------------\n",
            "ЁЯзк Example 7:\n",
            "ЁЯУЭ Original: рж╕рзЗ ржЦрзБржм ржкрж░рж┐рж╢рзНрж░ржорзА ржЫрж╛рждрзНрж░ред\n",
            "ЁЯзй Encoded IDs: [291, 270, 150, 273, 278, 435, 368, 275, 285, 283, 258, 155, 351, 275, 318]\n",
            "ЁЯФБ Decoded: рж╕рзЗ ржЦрзБржм ржкрж░рж┐рж╢рзНрж░ржорзА ржЫрж╛рждрзНрж░ред\n",
            "тЬЕ Match: True\n",
            "--------------------------------------------------------------------------------\n",
            "ЁЯзк Example 8:\n",
            "ЁЯУЭ Original: ржЖржорж╛ржжрзЗрж░ рж╕рзНржХрзБрж▓ рж╢рж╣рж░рзЗ ржЕржмрж╕рзНржерж┐рждред\n",
            "ЁЯзй Encoded IDs: [256, 134, 285, 344, 386, 263, 149, 352, 362, 311, 260, 439, 278, 432, 326, 318]\n",
            "ЁЯФБ Decoded: ржЖржорж╛ржжрзЗрж░ рж╕рзНржХрзБрж▓ рж╢рж╣рж░рзЗ ржЕржмрж╕рзНржерж┐рждред\n",
            "тЬЕ Match: True\n",
            "--------------------------------------------------------------------------------\n",
            "ЁЯзк Example 9:\n",
            "ЁЯУЭ Original: ржЦрж╛ржмрж╛рж░ржЯрж╛ ржжрж╛рж░рзБржг рж▓рзЗржЧрзЗржЫрзЗ!\n",
            "ЁЯзй Encoded IDs: [388, 330, 295, 299, 279, 166, 295, 273, 307, 258, 178, 271, 151, 348, 262, 33]\n",
            "ЁЯФБ Decoded: ржЦрж╛ржмрж╛рж░ржЯрж╛ ржжрж╛рж░рзБржг рж▓рзЗржЧрзЗржЫрзЗ!\n",
            "тЬЕ Match: True\n",
            "--------------------------------------------------------------------------------\n",
            "ЁЯзк Example 10:\n",
            "ЁЯУЭ Original: ржЖржЧрж╛ржорзАржХрж╛рж▓ рж╕ржХрж╛рж▓рзЗ рж╕рзВрж░рзНржпрзЛржжржпрж╝ рж╣ржмрзЗред\n",
            "ЁЯзй Encoded IDs: [256, 134, 306, 321, 283, 276, 308, 292, 276, 308, 375, 316, 260, 281, 286, 303, 284, 364, 278, 262, 318]\n",
            "ЁЯФБ Decoded: ржЖржЧрж╛ржорзАржХрж╛рж▓ рж╕ржХрж╛рж▓рзЗ рж╕рзВрж░рзНржпрзЛржжржпрж╝ рж╣ржмрзЗред\n",
            "тЬЕ Match: True\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "ЁЯОп Roundtrip Accuracy: 10/10 (100.0%)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "amLfkEjLXFfI"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}