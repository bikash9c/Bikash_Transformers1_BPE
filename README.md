# ðŸª¶ Bengali BPE Tokenizer

A custom **Byte Pair Encoding (BPE)** tokenizer trained on a **2 MB Bengali corpus**, designed for tokenizing and decoding Bengali text at a **byte level**.  
Built completely from scratch in **Python**, featuring a **reversible encoderâ€“decoder**, **sample Bengali text suggestions**, and a **round-trip validation** interface â€” deployed on **Hugging Face Spaces**.

---

## ðŸš€ Project Overview

This project demonstrates how to build a **byte-level BPE tokenizer (like GPT-2â€™s)** from scratch â€” trained specifically on Bengali UTF-8 text.  
It provides a **fully reversible tokenizer**, a **compact 5010-token vocabulary**, and a **visual Gradio web app** for testing and verification.

---

## ðŸ§© Key Features

- ðŸ”¤ **Byte-level BPE** â€” learns merges directly on UTF-8 bytes for full Bengali script coverage.  
- ðŸ” **Fully reversible encode/decode** â€” perfect reconstruction for clean text.  
- ðŸ§  **Round-trip test built-in** â€” automatically verifies that `decode(encode(text)) == text`.  
- ðŸ’¬ **Interactive Gradio UI** â€” includes sample Bengali sentences that auto-fill the input box.  
- âš™ï¸ **Compact 5010-token vocabulary** trained from a 2 MB corpus.  
- ðŸŒ **Deployable on Hugging Face Spaces** or runnable locally with Gradio.

---

## ðŸ“‚ Repository Structure

bengali-bpe-tokenizer/
â”‚
â”œâ”€â”€ bengali_tokenizer/
â”‚ â”œâ”€â”€ config.json # Metadata: vocab size, compression ratio, etc.
â”‚ â”œâ”€â”€ merges.txt # Integer merge pairs (BPE rules)
â”‚ â””â”€â”€ vocab.json # Token ID â†’ string mapping
â”‚
â”œâ”€â”€ app.py # Gradio web app with roundtrip + sample text buttons
â”œâ”€â”€ tokenizer.py # Core BengaliBPETokenizer class
â”œâ”€â”€ sample_bengali_text.txt # Example Bengali text for quick testing
â”œâ”€â”€ Bikash_Session_11_BPE.ipynb # Colab notebook used for training
â””â”€â”€ README.md # Project documentation



---

## ðŸ§  Training Summary (Final Version)

| Parameter | Value |
|------------|--------|
| **Corpus** | Bengali UTF-8 text (â‰ˆ 2 MB) |
| **Initial Tokens** | 2,097,152 bytes |
| **Target Vocab Size** | 5010 |
| **Base Tokens** | 0 â€“ 255 (byte IDs) |
| **Merged Tokens** | 256 â€“ 5009 |
| **Total Merges** | 4754 |
| **Training Method** | Full recomputation after every merge (no batching) |
| **Training Time** | â‰ˆ 13 minutes on Colab T4 |
| **Final Compression Ratio** | 9.54 Ã— |
| **Final Sequence Length** | 219,911 tokens (from 2,097,152 bytes) |

---

## ðŸ“Š Training Logs

Training on 2,097,152 byte tokens (~2 MB of data)

ðŸš€ Starting BPE training: 4754 merges (full recomputation each step)...

ðŸ“¦ Merges: 100/4754 | Current compression: 3.48Ã— | Current length: 602,858
ðŸ“¦ Merges: 500/4754 | Current compression: 5.29Ã— | Current length: 396,305
ðŸ“¦ Merges: 1000/4754 | Current compression: 6.35Ã— | Current length: 330,419
ðŸ“¦ Merges: 2000/4754 | Current compression: 7.61Ã— | Current length: 275,677
ðŸ“¦ Merges: 3000/4754 | Current compression: 8.46Ã— | Current length: 248,019
ðŸ“¦ Merges: 4000/4754 | Current compression: 9.12Ã— | Current length: 230,030
ðŸ“¦ Merges: 4754/4754 | Current compression: 9.54Ã— | Current length: 219,911

âœ… Training complete!
âœ… Final vocabulary size: 5010
âœ… Final compression ratio: 9.54Ã—
ðŸ“ Saved -> bengali_tokenizer/merges.txt
ðŸ“ Saved -> bengali_tokenizer/vocab.json
ðŸ“ Saved -> bengali_tokenizer/config.json





âš™ï¸ Tokenizer Files Explained
File	Description
merges.txt	Integer token-pair merges defining BPE rules
vocab.json	Token ID â†’ string mapping (Bengali glyphs + merged pairs)
config.json	Metadata â€” vocab size, compression ratio, notes

These three files fully define and reconstruct the tokenizer anywhere.

ðŸ§© Example Output (from Gradio App)
Input:

à¦¤à§‹à¦®à¦¾à¦° à¦®à¦™à§à¦—à¦² à¦¹à§‹à¦•! (à¦¹à¦¾à¦à¦šà¦¿ à¦¦à§‡à¦“à¦¯à¦¼à¦¾à¦° à¦¸à¦®à¦¯à¦¼)


Output:


ðŸ§© Token IDs (first 200):
[274, 286, 285, 302, 174, 256, 482, 277, 364, 286, ... ]

ðŸ”¤ Tokens (first 200):
['à¦¤', 'à§‹', 'à¦®', 'à¦¾', 'à¦°', ' ', 'à¦®', 'à¦‚', 'à¦—', 'à¦²', ' ', 'à¦¹', 'à§‹', 'à¦•', '!', ...]

ðŸ” Decoded text:
à¦¤à§‹à¦®à¦¾à¦° à¦®à¦™à§à¦—à¦² à¦¹à§‹à¦•! (à¦¹à¦¾à¦à¦šà¦¿ à¦¦à§‡à¦“à¦¯à¦¼à¦¾à¦° à¦¸à¦®à¦¯à¦¼)

ðŸ§  Roundtrip Test: âœ… True


ðŸ§° Requirements

python >= 3.9
gradio >= 4.0
tqdm
ðŸ“œ License
Released under the MIT License â€” free for research, educational, and commercial use with attribution.

ðŸ’« Hugging Face Space
ðŸ”— https://huggingface.co/spaces/bikash9c/bengali-tokenizer-bikash

ðŸª¶ Final Result
âœ… Efficient 5010-token Bengali Byte-Pair Encoding model

âœ… 9.54Ã— compression on 2 MB corpus

âœ… Perfect round-trip decode

âœ… Interactive Bengali tokenizer app with sample inputs and validation

Subword tokenization is where raw bytes learn to speak Bengali. ðŸª¶