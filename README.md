ğŸª¶ Bengali BPE Tokenizer

A custom Byte Pair Encoding (BPE) tokenizer trained on Bengali Wikipedia (~5 MB), designed for tokenizing and decoding Bengali text at a byte level. Built completely from scratch in Python and deployed with an interactive Gradio web app on Hugging Face Spaces.

ğŸš€ Project Overview

This project shows how to create a byte-level BPE tokenizer similar to GPT-2â€™s tokenizer, trained specifically for Bengali text data. It includes a reversible encoderâ€“decoder, Gradio UI, and deployment-ready tokenizer artifacts.

ğŸ§© Key Features

ğŸ”¤ Byte-level BPE â€” learns merges on UTF-8 bytes, ensuring full character coverage.

ğŸ” Fully reversible encodeâ€“decode process (perfect Bengali reconstruction).

âš™ï¸ Compact vocabulary (4.5 K) trained on 5 MB of Wikipedia text.

ğŸ’¬ Interactive Gradio app for testing tokenization visually.

ğŸŒ Deployable on Hugging Face Spaces or runnable locally.

ğŸ“‚ Repository Structure

bengali-bpe-tokenizer/
â”œâ”€â”€ app.py # Gradio interface
â”œâ”€â”€ tokenizer.py # Core tokenizer class (encode/decode)
â”œâ”€â”€ bengali_tokenizer/ # Trained tokenizer artifacts
â”‚ â”œâ”€â”€ merges.txt # Learned merge pairs
â”‚ â”œâ”€â”€ vocab.json # Token ID â†’ readable token mapping
â”‚ â””â”€â”€ config.json # Metadata
â”œâ”€â”€ train_tokenizer.ipynb # (Optional) Colab training script
â””â”€â”€ README.md # Project documentation

ğŸ§  Training Summary

The tokenizer was trained on Bengali Wikipedia (November 2023 dump) using 5 MB of UTF-8 text. The BPE algorithm iteratively merges the most frequent byte pairs, forming subword tokens.

Parameter	Value
Corpus	Bengali Wikipedia
Sample Size	5 MB
Vocabulary Size	4 500
Base Tokens	0â€“255 (byte IDs)
Merge Tokens	256â€“4 499
Final Compression Ratio	~3.29Ã—
ğŸ§© Usage Example

from tokenizer import BengaliBPETokenizer
tokenizer = BengaliBPETokenizer("bengali_tokenizer")
text = "à¦†à¦ªà¦¨à¦¾à¦•à§‡ à¦…à¦¨à§‡à¦• à¦§à¦¨à§à¦¯à¦¬à¦¾à¦¦à¥¤"
ids = tokenizer.encode(text)
decoded = tokenizer.decode(ids)
print("Token IDs:", ids)
print("Decoded Text:", decoded)

âœ… Output
Token IDs: [256, 134, 2656, 856, 4406, â€¦]
Decoded Text: à¦†à¦ªà¦¨à¦¾à¦•à§‡ à¦…à¦¨à§‡à¦• à¦§à¦¨à§à¦¯à¦¬à¦¾à¦¦à¥¤

ğŸ¨ Gradio Web App

The Gradio app provides an interactive interface to test tokenization and decoding. Users can enter Bengali text, view token IDs and subword segments, and verify that decoding returns the original text.

Run locally:
pip install gradio
python app.py

Or explore on Hugging Face Spaces:
ğŸ‘‰ Live Demo (Hugging Face): https://huggingface.co/spaces/bikash9c/BPE_bengali_tokenizer

âš™ï¸ Tokenizer Files Explained
File	Description
merges.txt	List of integer token-pair merges defining the BPE rules.
vocab.json	Maps token IDs to their readable representations (Bengali strings or merged pairs).
config.json	Stores metadata such as vocab size and compression ratio.

These three files are sufficient to reconstruct the tokenizer anywhere.

ğŸ§© Example Output (from Gradio app)

Input:
à¦¤à§‹à¦®à¦¾à¦° à¦®à¦™à§à¦—à¦² à¦¹à§‹à¦•! (à¦¹à¦¾à¦à¦šà¦¿ à¦¦à§‡à¦“à¦¯à¦¼à¦¾à¦° à¦¸à¦®à¦¯à¦¼)

Output:
ğŸ§© Token IDs (first 200): [1156, 1756, 1706, 2556, 174, 256, â€¦]
ğŸ”¤ Tokens (first 200): ['à¦¤à§‹', 'à¦®à¦¾', 'à¦°', ' ', 'à¦®', 'à¦™à§à¦—à¦²', â€¦]
ğŸ” Decoded text: à¦¤à§‹à¦®à¦¾à¦° à¦®à¦™à§à¦—à¦² à¦¹à§‹à¦•! (à¦¹à¦¾à¦à¦šà¦¿ à¦¦à§‡à¦“à¦¯à¦¼à¦¾à¦° à¦¸à¦®à¦¯à¦¼)

ğŸ§° Requirements

Runtime
python >= 3.9
gradio >= 4.0
tqdm

Optional (for training)
datasets

ğŸ“œ License

This project is released under the MIT License. You are free to use, modify, and integrate it into your NLP or research workflows.

âœï¸ Author

Bikash Chakraborty
Data Science & AI Enthusiast
ğŸ“§ your.email@example.com

ğŸŒ https://huggingface.co/your-username

Subword tokenization is where raw bytes learn to speak Bengali. ğŸª¶

ğŸ“Š Training Summary (from Logs)

Dataset: Bengali Wikipedia (5 MB sample, UTF-8 encoded)

Initial Tokens: 5,242,880 bytes

Target Vocabulary Size: 4,500

Total Merges: 4,244

Batch Size: 50 merges per recomputation

Total Batches: 85

Training Duration: ~36 minutes

Final Compression Ratio: 3.29Ã—

Final Sequence Length: 1,592,917 tokens (from 5,242,880)

Progress: Smooth and stable â€” compression steadily improved from 1.3Ã— to 3.29Ã—

Artifacts Saved:

âœ… /content/drive/MyDrive/bengali_tokenizer/merges.txt

âœ… /content/drive/MyDrive/bengali_tokenizer/vocab.json

âœ… /content/drive/MyDrive/bengali_tokenizer/config.json

Notes:

The training converged cleanly with a balanced tradeoff between vocabulary granularity and compression.

Step size of 50 maintained memory safety and efficient batching.

The tokenizer achieves full reversibility on Bengali text.

âœ… Final Result:
Efficient 4.5K-token Bengali Byte-Pair Encoding model with ~3.3Ã— compression and perfect decoding accuracy.
