ðŸª¶ Bengali BPE Tokenizer

A custom Byte Pair Encoding (BPE) tokenizer trained on Bengali Wikipedia (â‰ˆ 5 MB), designed for tokenizing and decoding Bengali text at a byte level.
Built entirely from scratch in Python, with a clean Gradio web app deployed on Hugging Face Spaces.

ðŸš€ Project Overview

This project demonstrates how to build a byte-level BPE tokenizer (like GPT-2â€™s) from scratch â€” trained specifically on Bengali UTF-8 text.
It features a reversible encoder-decoder, compact vocabulary, and an interactive web interface.

ðŸ§© Key Features

ðŸ”¤ Byte-level BPE â€” learns merges on UTF-8 bytes for full Bengali script coverage.

ðŸ” Fully reversible encode/decode (perfect Bengali reconstruction).

âš™ï¸ Compact vocabulary â€” 500 learned tokens from a 5 MB corpus.

ðŸ’¬ Interactive Gradio app for visual testing and verification.

ðŸŒ Deployable on Hugging Face Spaces or runnable locally.

ðŸ“‚ Repository Structure
bengali-bpe-tokenizer/
â”‚
â”œâ”€â”€ bengali_tokenizer/
â”‚   â”œâ”€â”€ config.json         # Metadata: vocab size, compression ratio, etc.
â”‚   â”œâ”€â”€ merges.txt          # Integer merge pairs (BPE rules)
â”‚   â””â”€â”€ vocab.json          # Token ID â†’ string mapping
â”‚
â”œâ”€â”€ app.py                  # Gradio web app interface
â”œâ”€â”€ tokenizer.py            # Core BengaliBPETokenizer class
â”œâ”€â”€ sample_bengali_text.txt # Example Bengali text for quick testing
â”œâ”€â”€ Bikash_Session_11_BPE.ipynb  # Colab notebook used for training
â””â”€â”€ README.md               # Project documentation

ðŸ§  Training Summary (Final Version)
Parameter	Value
Corpus	Bengali Wikipedia (5 MB UTF-8 sample)
Initial Tokens	5 ,242 ,880 bytes
Target Vocab Size	500
Base Tokens	0 â€“ 255 (byte IDs)
Merged Tokens	256 â€“ 499
Total Merges	244
Training Method	Full recomputation after every merge (no batching)
Training Time	â‰ˆ 3.5 minutes on Colab T4
Final Compression Ratio	4.25 Ã—
Final Sequence Length	1 ,234 ,765 tokens (from 5 ,242 ,880 bytes)
ðŸ“Š Training Logs
Training on 5,242,880 byte tokens (~5 MB of data)
ðŸš€ Starting BPE training: 244 merges (full recomputation each step)...

ðŸ“¦ Merges: 100/244 | Compression: 3.42Ã— | Length: 1,533,033
ðŸ“¦ Merges: 200/244 | Compression: 4.04Ã— | Length: 1,296,602
ðŸ“¦ Merges: 244/244 | Compression: 4.25Ã— | Length: 1,234,765

âœ… Training complete!
âœ… Final vocabulary size: 500
âœ… Final compression ratio: 4.25Ã—
ðŸ“ Saved -> bengali_tokenizer/merges.txt
ðŸ“ Saved -> bengali_tokenizer/vocab.json
ðŸ“ Saved -> bengali_tokenizer/config.json

ðŸ§© Usage Example
from tokenizer import BengaliBPETokenizer

tokenizer = BengaliBPETokenizer("bengali_tokenizer")

text = "à¦†à¦ªà¦¨à¦¾à¦•à§‡ à¦…à¦¨à§‡à¦• à¦§à¦¨à§à¦¯à¦¬à¦¾à¦¦à¥¤"
ids = tokenizer.encode(text)
decoded = tokenizer.decode(ids)

print("Token IDs:", ids)
print("Decoded Text:", decoded)


âœ… Output

Token IDs: [256, 134, 2656, 856, 4406, â€¦]
Decoded Text: à¦†à¦ªà¦¨à¦¾à¦•à§‡ à¦…à¦¨à§‡à¦• à¦§à¦¨à§à¦¯à¦¬à¦¾à¦¦à¥¤

ðŸŽ¨ Gradio Web App

The Gradio app provides an interactive Bengali tokenization demo.
Enter Bengali text or load a sample, view token IDs and merged tokens, and verify that decoding reconstructs the original text.

â–¶ï¸ Run locally
pip install gradio
python app.py

ðŸŒ Live on Hugging Face Spaces

ðŸ‘‰ Bengali BPE Tokenizer Demo

âš™ï¸ Tokenizer Files Explained
File	Description
merges.txt	List of integer token-pair merges defining BPE rules.
vocab.json	Token ID â†’ string (Bengali glyphs or merged pairs).
config.json	Metadata such as vocab size and compression ratio.

These three files are sufficient to reconstruct the tokenizer anywhere.

ðŸ§© Example Output (from Gradio App)

Input:

à¦¤à§‹à¦®à¦¾à¦° à¦®à¦™à§à¦—à¦² à¦¹à§‹à¦•! (à¦¹à¦¾à¦à¦šà¦¿ à¦¦à§‡à¦“à¦¯à¦¼à¦¾à¦° à¦¸à¦®à¦¯à¦¼)


Output:

ðŸ§© **Token IDs (first 200):**
[274, 286, 285, 302, 174, 256, 482, 277, 364, 286, 276, 33, 374, 311, 265, 129, 359, 290, 166, 271, 147, 284, 302, 184, 285, 284, 41] ...

ðŸ”¤ **Tokens (first 200):**
['à¦¤', 'à§‹', 'à¦®', 'Ã Â¦Â¾Ã Â¦Â° Ã Â¦', 'Â®', 'Ã Â¦', '\x99Ã Â§\x8dÃ Â¦\x97', 'à¦²', ' à¦¹', 'à§‹', 'à¦•', '!', ' (', 'à¦¹', 'Ã Â¦Â¾Ã Â¦', '\x81', 'à¦š', 'Ã Â¦Â¿ Ã Â¦', 'Â¦', 'Ã Â§\x87Ã Â¦', '\x93', 'à¦¯à¦¼', 'Ã Â¦Â¾Ã Â¦Â° Ã Â¦', 'Â¸', 'à¦®', 'à¦¯à¦¼', ')']

ðŸ” **Decoded text:**
à¦¤à§‹à¦®à¦¾à¦° à¦®à¦™à§à¦—à¦² à¦¹à§‹à¦•! (à¦¹à¦¾à¦à¦šà¦¿ à¦¦à§‡à¦“à¦¯à¦¼à¦¾à¦° à¦¸à¦®à¦¯à¦¼)

ðŸ§° Requirements

python >= 3.9
gradio >= 4.0
tqdm



ðŸ“œ License

Released under the MIT License â€” free for research, educational, and commercial use with attribution.


Hugging face link:
https://huggingface.co/spaces/bikash9c/bengali-tokenizer-bikash

Subword tokenization is where raw bytes learn to speak Bengali. ðŸª¶

âœ… Final Result

Efficient 500-token Bengali Byte-Pair Encoding model

4.25Ã— compression

Perfect decode round-trip

Compact & deployable tokenizer for Bengali language processing ðŸš€